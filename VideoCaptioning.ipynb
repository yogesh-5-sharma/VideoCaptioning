{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "class CNN():\n",
    "  def __init__(self):\n",
    "    model_directory = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    if not os.path.exists(model_directory):\n",
    "      vggmodel = VGG16(weights= 'imagenet')\n",
    "    else:\n",
    "      vggmodel = VGG16(weights= model_directory)\n",
    "    self.model = tf.keras.models.Model(vggmodel.input, vggmodel.layers[-2].output)\n",
    "\n",
    "  def __preprocess_frames(self, frames):\n",
    "    frames = np.array(list(map(lambda x: cv2.resize(x, (224,224)), frames)))\n",
    "    preprocessed_frames = preprocess_input(frames)\n",
    "    return preprocessed_frames\n",
    "  \n",
    "  def extract_features(self, frames):\n",
    "    preprocessed_frames = self.__preprocess_frames(frames)\n",
    "    features = self.model.predict(preprocessed_frames)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import heapq\n",
    "import cv2\n",
    "\n",
    "class Video_Caption_Generator():\n",
    "  def __init__(self):\n",
    "    \n",
    "    self.saved_model_directory = 'saved_models'\n",
    "    if not os.path.exists(os.path.join(self.saved_model_directory, 'encoder_model.h5')) or \\\n",
    "       not os.path.exists(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5')):\n",
    "      raise Exception(\"No trained models found. Check for correct model filenames if already trained, else train model first.\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(self.saved_model_directory, 'tokenizer.json')):\n",
    "      raise Exception(\"Tokenizer object not found.\")\n",
    "    \n",
    "    self.num_tokens_decoder = 5000\n",
    "    self.latent_dims = 1000\n",
    "    self.frames_limit = 80\n",
    "    \n",
    "    with open(os.path.join(self.saved_model_directory, 'tokenizer.json')) as fp:\n",
    "      tokenizer_json = json.load(fp)\n",
    "    \n",
    "    self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
    "    self.encoder_model, self.decoder_model = self.__get_inference_model()\n",
    "    self.cnn_model = CNN()\n",
    "    \n",
    "    self.caption_limit = 30\n",
    "    \n",
    "    self.testing_features_directory = 'dataset\\yt_allframes_vgg_fc7_test.txt'\n",
    "    self.testing_sents_directory = 'dataset\\sents_test_lc_nopunc.txt'\n",
    "    \n",
    "  def __get_inference_model(self):\n",
    "    e_model = tf.keras.models.load_model(os.path.join(self.saved_model_directory, 'encoder_model.h5'))\n",
    "\n",
    "    d_input = tf.keras.layers.Input(shape= (None, self.num_tokens_decoder))\n",
    "    d_input_h = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    d_input_c = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    d_input_state = [d_input_h, d_input_c]\n",
    "    d_lstm = tf.keras.layers.LSTM(self.latent_dims, return_state= True, return_sequences= True)\n",
    "    d_dense = tf.keras.layers.Dense(units= self.num_tokens_decoder, activation= 'softmax')\n",
    "    \n",
    "    d_output, d_state_h, d_state_c = d_lstm(d_input, initial_state= d_input_state)\n",
    "    d_output_state = [d_state_h, d_state_c]\n",
    "    d_output = d_dense(d_output)\n",
    "    d_model = tf.keras.models.Model(inputs= [d_input, d_input_state], outputs= [d_output, d_output_state])\n",
    "    d_model.load_weights(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5'))\n",
    "\n",
    "    return e_model, d_model\n",
    "  \n",
    "  def __convert_sequence_to_text(self, candidate_captions, beam_width):\n",
    "    candidate_captions = list(map(lambda x: (x[0],x[1]), candidate_captions))\n",
    "    candidate_captions = sorted(candidate_captions, key= lambda x: x[0], reverse= True)\n",
    "    resulting_captions = []\n",
    "    resulting_probabilities = []\n",
    "    for candidate in candidate_captions:\n",
    "      sequence= candidate[1]\n",
    "      sequence= sequence[1:]\n",
    "      if self.tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "        sequence = sequence[:-1]\n",
    "      if self.tokenizer.index_word[sequence[-1]] == \"a\" and beam_width <=5:\n",
    "        sequence = sequence[:-1]\n",
    "      caption = self.tokenizer.sequences_to_texts([sequence])[0]\n",
    "      resulting_captions.append(caption)\n",
    "      resulting_probabilities.append(candidate[0])\n",
    "    \n",
    "    return resulting_captions, resulting_probabilities\n",
    "  \n",
    "  def __generate_captions(self, video_features, beam_width):\n",
    "    video_features = np.expand_dims(video_features, axis=0)\n",
    "    state_values = self.encoder_model.predict(video_features)\n",
    "    candidate_captions = [[0.0, [self.tokenizer.word_index['<bos>']], state_values]]\n",
    "    \n",
    "    for i in range(self.caption_limit):\n",
    "      new_candidates = []\n",
    "      all_done = 1\n",
    "      for candidate in candidate_captions:\n",
    "        prob = candidate[0]\n",
    "        sequence = candidate[1]\n",
    "        state = candidate[2]\n",
    "        if self.tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "          if len(new_candidates) < beam_width:\n",
    "            heapq.heappush(new_candidates, (prob, sequence, state))\n",
    "          elif prob > new_candidates[0][0]:\n",
    "            heapq.heappushpop(new_candidates, (prob, sequence, state))\n",
    "          continue\n",
    "        all_done = 0\n",
    "        decoder_input_data = np.zeros(shape = (1, 1, self.num_tokens_decoder))\n",
    "        decoder_input_data[0, 0, sequence[-1]] = 1\n",
    "        probabilities, state_output = self.decoder_model.predict([decoder_input_data, state])\n",
    "        for j in range(1, self.num_tokens_decoder):\n",
    "          if probabilities[0, 0, j] > 0.0 and j != sequence[-1]:\n",
    "            new_prob = (prob*len(sequence) + math.log(probabilities[0, 0, j]))/(1+len(sequence))\n",
    "            new_sequence = sequence.copy()\n",
    "            new_sequence.append(j)\n",
    "            if len(new_candidates) < beam_width:\n",
    "              heapq.heappush(new_candidates, (new_prob, new_sequence, state_output))\n",
    "            elif new_prob > new_candidates[0][0]:\n",
    "              heapq.heappushpop(new_candidates, (new_prob, new_sequence, state_output))\n",
    "      \n",
    "      if all_done == 1:\n",
    "        break\n",
    "      candidate_captions = new_candidates.copy()\n",
    "    \n",
    "    return self.__convert_sequence_to_text(candidate_captions, beam_width)\n",
    "  \n",
    "  def get_video_captions(self, video_directory, beam_width=5):\n",
    "    \n",
    "    if not os.path.exists(video_directory):\n",
    "      raise Exception(\"Invalid Video directory\")\n",
    "    \n",
    "    print(\"Reading Video Frames...\")\n",
    "    cap = cv2.VideoCapture(video_directory)\n",
    "    \n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    if frames.shape[0] > self.frames_limit:\n",
    "      frames_idx = np.linspace(0, frames.shape[0]-1, self.frames_limit).astype('int')\n",
    "      frames = frames[frames_idx]\n",
    "    \n",
    "    print(\"Extracting Features...\")\n",
    "    features = self.cnn_model.extract_features(frames)\n",
    "    features = np.pad(features, ((0,self.frames_limit - features.shape[0]), (0,0)))\n",
    "    \n",
    "    print(\"Generating Captions...\")\n",
    "    captions, probabilities = self.__generate_captions(features, beam_width)\n",
    "    \n",
    "    return captions\n",
    "  \n",
    "  def get_bleu_score(self, references, hypothesis):\n",
    "    references = list(map(lambda x: x.split(), references))\n",
    "    hypothesis = list(map(lambda x: x.split(), hypothesis))\n",
    "    bleu_scores = []\n",
    "    for hypo in hypothesis:\n",
    "      score = nltk.translate.bleu_score.sentence_bleu(references, hypo)\n",
    "      bleu_scores.append(score)\n",
    "    return max(bleu_scores)\n",
    "  \n",
    "  def get_meteor_score(self, references, hypothesis):\n",
    "    try:\n",
    "      nltk.data.find('corpora\\wordnet')\n",
    "    except LookupError:\n",
    "      nltk.download('wordnet')\n",
    "      \n",
    "    meteor_scores = []\n",
    "    for hypo in hypothesis:\n",
    "      score = nltk.translate.meteor_score.meteor_score(references, hypo)\n",
    "      meteor_scores.append(score)\n",
    "    return max(meteor_scores)\n",
    "  \n",
    "  def __getFeatures(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split()\n",
    "    features = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split(',')\n",
    "      id = row[0].split('_')[0]\n",
    "      if id not in features:\n",
    "        features[id]=[]\n",
    "      features[id].append(np.asarray(row[1:], dtype=np.float))\n",
    "\n",
    "    for x in features:\n",
    "      features[x] = np.array(features[x])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "  def __getSents(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split('\\n')\n",
    "    sents = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split('\\t')\n",
    "      if len(row)<2:\n",
    "        continue\n",
    "      id = row[0]\n",
    "      if id not in sents:\n",
    "        sents[id] = []\n",
    "      sents[id].append(row[1])\n",
    "\n",
    "    return sents\n",
    "  \n",
    "  def test(self, beam_width_values):\n",
    "    testing_features = self.__getFeatures(self.testing_features_directory)\n",
    "    testing_sents = self.__getSents(self.testing_sents_directory)\n",
    "    for key in testing_features:\n",
    "      vid_features = testing_features[key]\n",
    "      if vid_features.shape[0] < self.frames_limit:\n",
    "        vid_features = np.pad(vid_features, ((0, self.frames_limit-vid_features.shape[0]), (0,0)))\n",
    "      else:\n",
    "        frames_idx = np.linspace(0, vid_features.shape[0]-1, self.frames_limit).astype('int')\n",
    "        vid_features = vid_features[frames_idx]\n",
    "      testing_features[key] = vid_features\n",
    "    \n",
    "    bleu = []\n",
    "    meteor = []\n",
    "    for beam_width in beam_width_values:\n",
    "      print(\"!!\",beam_width)\n",
    "      bleu_temp = []\n",
    "      meteor_temp = []\n",
    "      for key, features in testing_features.items():\n",
    "        captions = self.__generate_captions_optimized(features, beam_width)[0]\n",
    "        bleu_score = self.get_bleu_score(testing_sents[key], captions)\n",
    "        meteor_score = self.get_meteor_score(testing_sents[key], captions)\n",
    "        bleu_temp.append(bleu_score)\n",
    "        meteor_temp.append(meteor_score)\n",
    "      bleu.append(np.average(bleu_temp))\n",
    "      meteor.append(np.average(meteor_temp))\n",
    "    return bleu, meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "vcg = Video_Caption_Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Video Frames...\n",
      "Extracting Features...\n",
      "Generating Captions...\n",
      "Captions generated are:\n",
      "0. a chipmunk is talking\n",
      "1. a chipmunk is eating\n",
      "2. a chipmunk is drinking\n",
      "3. a chipmunk is drawing\n",
      "4. a chipmunk is talking\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\Yogesh\\Desktop\\Projects\\Video Captioning\\dataset\\demo\\11.gif'\n",
    "beam_width = 5\n",
    "captions = vcg.get_video_captions(path, beam_width)\n",
    "print(\"Captions generated are:\")\n",
    "for i, caption in enumerate(captions):\n",
    "  print(str(i)+\".\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width_values = [1, 2, 3, 4, 5, 10, 15, 20]\n",
    "bleu, meteor = vcg.test(beam_width_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, batch_size, features, video_ids, cap_sequences, vocab_size, max_length):\n",
    "    self.batch_size = batch_size\n",
    "    self.features = features\n",
    "    self.video_ids = video_ids\n",
    "    self.cap_sequences = cap_sequences\n",
    "    self.vocab_size = vocab_size\n",
    "    self.max_len = max_length\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indices = np.arange(len(self.cap_sequences))\n",
    "    np.random.shuffle(self.indices)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.indices)//self.batch_size\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    idx_range = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "    return self.__data_generation(idx_range)\n",
    "  \n",
    "  def __data_generation(self, list_index):\n",
    "    X1=[]\n",
    "    X2=[]\n",
    "    Y=[]\n",
    "    for i in list_index:\n",
    "      id=self.video_ids[i]\n",
    "      X1.append(self.features[id])\n",
    "      X2.append(tf.keras.utils.to_categorical(self.cap_sequences[i][:-1],num_classes= self.vocab_size))\n",
    "      Y.append(tf.keras.utils.to_categorical(self.cap_sequences[i][1:],num_classes= self.vocab_size))\n",
    "    X1=np.array(X1)\n",
    "    X2=tf.keras.preprocessing.sequence.pad_sequences(X2, maxlen= self.max_len, padding='post')\n",
    "    Y=tf.keras.preprocessing.sequence.pad_sequences(Y, maxlen= self.max_len, padding='post')\n",
    "    return [X1,X2], Y\n",
    "\n",
    "\n",
    "\n",
    "class Video_Captioning_Model():\n",
    "  def __init__(self, ):\n",
    "    \n",
    "    training_features_directory = 'dataset\\yt_allframes_vgg_fc7_train.txt'\n",
    "    training_sents_directory = 'dataset\\sents_train_lc_nopunc.txt'\n",
    "    validation_features_directory = 'dataset\\yt_allframes_vgg_fc7_val.txt'\n",
    "    validation_sents_directory = 'dataset\\sents_val_lc_nopunc.txt'\n",
    "    \n",
    "    print(\"Loading Dataset...\")\n",
    "    raw_training_features = self.__getFeatures(training_features_directory)\n",
    "    raw_training_sents = self.__getSents(training_sents_directory)\n",
    "    raw_validation_features = self.__getFeatures(validation_features_directory)\n",
    "    raw_validation_sents = self.__getSents(validation_sents_directory)\n",
    "    \n",
    "    self.frames_limit = 80\n",
    "    self.vocab_size = 5000\n",
    "    \n",
    "    self.saved_model_directory = 'saved_models'\n",
    "    \n",
    "    self.training_features, self.training_ids, self.training_captions = self.preprocess_data(raw_training_features,\n",
    "                                                                                             raw_training_sents)\n",
    "    self.validation_features, self.validation_ids, self.validation_captions = self.preprocess_data(raw_validation_features, \n",
    "                                                                                                   raw_validation_sents)\n",
    "    \n",
    "    self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words= self.vocab_size,\n",
    "                                                          filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                          oov_token= \"<oov>\")\n",
    "    self.tokenizer.fit_on_texts(self.training_captions + self.validation_captions)\n",
    "    self.training_seq = self.tokenizer.texts_to_sequences(self.training_captions)\n",
    "    self.validation_seq = self.tokenizer.texts_to_sequences(self.validation_captions)\n",
    "    self.max_length = max([len(x) for x in self.training_seq])\n",
    "    \n",
    "    tokenizer_json = self.tokenizer.to_json()\n",
    "    with open(os.path.join(self.saved_model_directory, 'tokenizer.json'), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "    self.num_timesteps_encoder = 80\n",
    "    self.num_tokens_encoder = 4096\n",
    "    self.latent_dims = 1000\n",
    "    self.num_timesteps_decoder = self.max_length - 1\n",
    "    self.num_tokens_decoder = self.vocab_size\n",
    "    \n",
    "    return\n",
    "  \n",
    "  def train(self):\n",
    "    self.model = self.build_model()\n",
    "    self.model.compile(\n",
    "      optimizer= 'adam',\n",
    "      loss= 'categorical_crossentropy',\n",
    "      metrics= ['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Creating Generators...\")\n",
    "    training_generator = DataGenerator(\n",
    "        batch_size= 32,\n",
    "        features= self.training_features,\n",
    "        video_ids= self.training_ids,\n",
    "        cap_sequences= self.training_seq,\n",
    "        max_length= self.max_length-1,\n",
    "        vocab_size= self.vocab_size\n",
    "    )\n",
    "\n",
    "    validation_generator = DataGenerator(\n",
    "      batch_size= 32,\n",
    "      features= self.validation_features,\n",
    "      video_ids= self.validation_ids,\n",
    "      cap_sequences= self.validation_seq,\n",
    "      max_length= self.max_length-1,\n",
    "      vocab_size= self.vocab_size\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    try:\n",
    "      self.history = self.model.fit(\n",
    "        training_generator,\n",
    "        epochs= 500,\n",
    "        #validation_data = validation_generator\n",
    "      )\n",
    "    except KeyboardInterrupt:\n",
    "      print(\"Keyboard interrupt!!\")\n",
    "    \n",
    "    print(\"Saving Models...\")\n",
    "    self.model.save(os.path.join(self.saved_model_directory, 'model.h5'))\n",
    "\n",
    "    self.encoder_model = tf.keras.models.Model(self.encoder_input, self.encoder_state)\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_output_lstm, state_h, state_c = self.decoder_lstm(self.decoder_input, initial_state= decoder_state_inputs)\n",
    "    decoder_state_outputs = [state_h, state_c]\n",
    "    decoder_output_dense = self.decoder_dense(decoder_output_lstm)\n",
    "    self.decoder_model = tf.keras.models.Model(inputs=[self.decoder_input, decoder_state_inputs], outputs=[decoder_output_dense,decoder_state_outputs])\n",
    "\n",
    "    self.encoder_model.save(os.path.join(self.saved_model_directory, 'encoder_model.h5'))\n",
    "    self.decoder_model.save_weights(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5'))\n",
    "    np.save(os.path.join(self.saved_model_directory, 'history.npy'), self.history.history)\n",
    "    print(\"Done\")\n",
    "\n",
    "  def retrain(self):\n",
    "    self.model = tf.keras.models.load_model(os.path.join(self.saved_model_directory, 'model.h5'))\n",
    "    \n",
    "    print(\"Creating Generators...\")\n",
    "    training_generator = DataGenerator(\n",
    "        batch_size= 32,\n",
    "        features= self.training_features,\n",
    "        video_ids= self.training_ids,\n",
    "        cap_sequences= self.training_seq,\n",
    "        max_length= self.max_length-1,\n",
    "        vocab_size= self.vocab_size\n",
    "    )\n",
    "\n",
    "    validation_generator = DataGenerator(\n",
    "      batch_size= 32,\n",
    "      features= self.validation_features,\n",
    "      video_ids= self.validation_ids,\n",
    "      cap_sequences= self.validation_seq,\n",
    "      max_length= self.max_length-1,\n",
    "      vocab_size= self.vocab_size\n",
    "    )\n",
    "    \n",
    "    print(\"Re-Starting Training...\")\n",
    "    try:\n",
    "      self.history = self.model.fit(\n",
    "        training_generator,\n",
    "        epochs= 1,\n",
    "        #validation_data = validation_generator\n",
    "      )\n",
    "    except KeyboardInterrupt:\n",
    "      print(\"Keyboard interrupt!!\")\n",
    "    \n",
    "    print(\"Saving Models...\")\n",
    "    self.model.save(os.path.join(self.saved_model_directory, 'model.h5'))\n",
    "\n",
    "    self.encoder_model = tf.keras.models.Model(self.encoder_input, self.encoder_state)\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_output_lstm, state_h, state_c = self.decoder_lstm(self.decoder_input, initial_state= decoder_state_inputs)\n",
    "    decoder_state_outputs = [state_h, state_c]\n",
    "    decoder_output_dense = self.decoder_dense(decoder_output_lstm)\n",
    "    self.decoder_model = tf.keras.models.Model(inputs=[self.decoder_input, decoder_state_inputs], outputs=[decoder_output_dense,decoder_state_outputs])\n",
    "\n",
    "    self.encoder_model.save(os.path.join(self.saved_model_directory, 'encoder_model.h5'))\n",
    "    self.decoder_model.save_weights(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5'))\n",
    "    np.save(os.path.join(self.saved_model_directory, 'history1.npy'), self.history.history)\n",
    "    print(\"Done\")\n",
    "  \n",
    "  def build_model(self):\n",
    "    print(\"Building Model...\")\n",
    "    \n",
    "    self.encoder_input = tf.keras.layers.Input(shape = (self.num_timesteps_encoder, self.num_tokens_encoder), name= 'encoder_inputs')\n",
    "    self.encoder_lstm = tf.keras.layers.LSTM(units= self.latent_dims, return_state= True, return_sequences= True, name= 'encoder_lstm')\n",
    "    _, state_h, state_c = self.encoder_lstm(self.encoder_input)\n",
    "    self.encoder_state = [state_h, state_c]\n",
    "    \n",
    "    self.decoder_input = tf.keras.layers.Input(shape = (self.num_timesteps_decoder, self.num_tokens_decoder), name= 'decoder_inputs')\n",
    "    self.decoder_lstm = tf.keras.layers.LSTM(units= self.latent_dims, return_state= True, return_sequences= True, name= 'decoder_lstm')\n",
    "    self.decoder_output, _, _ = self.decoder_lstm(self.decoder_input, initial_state= self.encoder_state)\n",
    "    self.decoder_dense = tf.keras.layers.Dense(units= self.vocab_size, activation= 'softmax', name= 'decoder_dense')\n",
    "    self.decoder_output = self.decoder_dense(self.decoder_output)\n",
    "    \n",
    "    model = tf.keras.models.Model([self.encoder_input, self.decoder_input], self.decoder_output)\n",
    "    return model\n",
    "  \n",
    "  def preprocess_data(self, features, sents):\n",
    "    result_features = {}\n",
    "    result_ids = []\n",
    "    result_captions = []\n",
    "    \n",
    "    for x,y in features.items():\n",
    "      video_features = None\n",
    "      if y.shape[0] > self.frames_limit:\n",
    "        idx = np.linspace(0, y.shape[0]-1, self.frames_limit).astype('int')\n",
    "        video_features = y[idx]\n",
    "      else:\n",
    "        video_features = np.pad(y, ((0, self.frames_limit - y.shape[0]), (0, 0)))\n",
    "      result_features[x]=video_features\n",
    "    \n",
    "    for id, captions in sents.items():\n",
    "      for caption in captions:\n",
    "        result_ids.append(id)\n",
    "        cap = \"<bos> \" + caption + \" <eos>\"\n",
    "        result_captions.append(cap)\n",
    "    \n",
    "    return result_features, result_ids, result_captions\n",
    "  \n",
    "  def __getFeatures(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split()\n",
    "    features = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split(',')\n",
    "      id = row[0].split('_')[0]\n",
    "      if id not in features:\n",
    "        features[id]=[]\n",
    "      features[id].append(np.asarray(row[1:], dtype=np.float))\n",
    "\n",
    "    for x in features:\n",
    "      features[x] = np.array(features[x])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "  def __getSents(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split('\\n')\n",
    "    sents = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split('\\t')\n",
    "      if len(row)<2:\n",
    "        continue\n",
    "      id = row[0]\n",
    "      if id not in sents:\n",
    "        sents[id] = []\n",
    "      sents[id].append(row[1])\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcm = Video_Captioning_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = r'C:\\Users\\Yogesh\\Desktop\\Projects\\Video Captioning\\dataset\\demo\\15.mp4'\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "while cap.isOpened():\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    break\n",
    "  cv2.imshow('video',frame)\n",
    "  if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitcfd7c34c2f954e0c9a347e48585b174c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
