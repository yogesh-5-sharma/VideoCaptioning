{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_video():\n",
    "  frames = np.random.randint(30,250)\n",
    "  width = np.random.randint(200, 1024)\n",
    "  height = np.random.randint(200,1024)\n",
    "  return np.random.randint(0, 256, (frames, width, height, 3), dtype= np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_video_features(features_size):\n",
    "  frames = np.random.randint(30, 250)\n",
    "  frames = 80\n",
    "  return 15.0*np.random.random_sample((frames, features_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video_frames(frames, frames_limit):\n",
    "  frames = np.array(frames)\n",
    "  selected_frames = frames\n",
    "  if frames.shape[0] > frames_limit:\n",
    "    idx = np.linspace(0, len(frames)-1, frames_limit).astype('int')\n",
    "    selected_frames = frames[idx]\n",
    "\n",
    "  selected_frames = np.array(list(map(lambda x: cv2.resize(x,(224,224)), selected_frames)))\n",
    "  preprocessed_frames = tf.keras.applications.vgg16.preprocess_input(selected_frames)\n",
    "  return preprocessed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 736, 728, 3)\n"
     ]
    }
   ],
   "source": [
    "video = get_random_video()\n",
    "print(video.shape)\n",
    "preprocessed_images = preprocess_video_frames(video, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(video_features, beam_width):\n",
    "  video_features = np.expand_dims(video_features, axis=0)\n",
    "  state_values = emodel.predict(video_features)\n",
    "  caption_limit = 30\n",
    "  candidate_captions = [[ 0.0, [tokenizer.word_index['<bos>']], state_values ]]\n",
    "\n",
    "  for i in range(caption_limit):\n",
    "    new_candidates = []\n",
    "    alldone=1\n",
    "    for candidate in candidate_captions:\n",
    "      prob = candidate[0]\n",
    "      state = candidate[2]\n",
    "      sequence = candidate[1]\n",
    "      if tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "        new_candidates.append([prob, sequence, state])\n",
    "        continue\n",
    "      alldone=0\n",
    "      decoder_input_data = np.zeros((1,1,vocab_size))\n",
    "      decoder_input_data[0,0, sequence[-1]] = 1\n",
    "      probabilities, state_output = dmodel.predict([decoder_input_data, state])\n",
    "      for i in range(1,vocab_size):\n",
    "        if probabilities[0,0,i] > 0.0:\n",
    "          new_prob = prob + math.log(probabilities[0,0,i])\n",
    "          new_sequence = sequence.copy()\n",
    "          new_sequence.append(i)\n",
    "          new_candidates.append([new_prob, new_sequence, state_output])\n",
    "    if alldone==1:\n",
    "      break\n",
    "    candidate_captions = sorted(new_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "  result = []\n",
    "  for caption in candidate_captions:\n",
    "    text_caption = tokenizer.sequences_to_texts([caption[1]])[0]\n",
    "    result.append([text_caption, caption[0]])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_optimized(video_features, beam_width):\n",
    "  video_features = np.expand_dims(video_features, axis=0)\n",
    "  state_values = emodel.predict(video_features)\n",
    "  caption_limit = 30\n",
    "  candidate_captions = [( 0.0, [tokenizer.word_index['<bos>']], state_values )]\n",
    "\n",
    "  for i in range(caption_limit):\n",
    "    new_candidates = []\n",
    "    alldone=1\n",
    "    for candidate in candidate_captions:\n",
    "      prob = candidate[0]\n",
    "      state = candidate[2]\n",
    "      sequence = candidate[1]\n",
    "      if tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "        if len(new_candidates) < beam_width:\n",
    "          heapq.heappush(new_candidates, (prob, sequence, state))\n",
    "        elif prob > new_candidates[0][0]:\n",
    "          heapq.heappushpop(new_candidates, (prob, sequence, state))\n",
    "        continue\n",
    "      alldone=0\n",
    "      decoder_input_data = np.zeros((1,1,vocab_size))\n",
    "      decoder_input_data[0,0, sequence[-1]] = 1\n",
    "      probabilities, state_output = dmodel.predict([decoder_input_data, state])\n",
    "      for i in range(1,vocab_size):\n",
    "        if probabilities[0,0,i] > 0.0:\n",
    "          new_prob = prob + math.log(probabilities[0,0,i])\n",
    "          new_prob = new_prob/(1+len(sequence))\n",
    "          new_sequence = sequence.copy()\n",
    "          new_sequence.append(i)\n",
    "          if len(new_candidates) < beam_width:\n",
    "            heapq.heappush(new_candidates, (new_prob, new_sequence, state_output))\n",
    "          elif new_prob > new_candidates[0][0]:\n",
    "            heapq.heappushpop(new_candidates, (new_prob, new_sequence, state_output))\n",
    "    if alldone==1:\n",
    "      break\n",
    "    candidate_captions = new_candidates.copy()\n",
    "\n",
    "  result = []\n",
    "  for caption in candidate_captions:\n",
    "    text_caption = tokenizer.sequences_to_texts([caption[1]])[0]\n",
    "    result.append([text_caption, caption[0]])\n",
    "  result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> a man is <eos>', -5.683006892212553]\n",
      "['<bos> a woman is <eos>', -6.40756073574623]\n",
      "['<bos> a man is a <eos>', -6.891812276544744]\n",
      "['<bos> a man is dancing <eos>', -7.6092122876976225]\n",
      "['<bos> a man is is <eos>', -7.627943971628076]\n",
      "['<bos> a woman is a <eos>', -7.633554987435413]\n",
      "['<bos> a man is slicing <eos>', -8.006814813764306]\n",
      "['<bos> a man is a a <eos>', -8.120699711445491]\n",
      "['<bos> a woman is a a <eos>', -8.861343497040727]\n",
      "['<bos> a man is is a <eos>', -8.868172121479288]\n",
      "['<bos> a man is dancing a <eos>', -9.002071831413854]\n",
      "['<bos> a man is slicing a <eos>', -9.092909356349743]\n",
      "['<bos> a is is a a <eos>', -9.309703334268905]\n",
      "['<bos> a girl is a a <eos>', -9.433857788981967]\n",
      "['<bos> a man is a the <eos>', -9.45685188809619]\n",
      "['<bos> a man is the a <eos>', -9.475865581836507]\n",
      "['<bos> a man is down a <eos>', -9.534361909321367]\n",
      "['<bos> a man is a a a <eos>', -10.11085002800423]\n",
      "['<bos> a man is is a a <eos>', -10.777907793414395]\n",
      "['<bos> a woman is a a a <eos>', -10.841952785389989]\n",
      "10.607357740402222\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "video_features = get_random_video_features(4096)\n",
    "starttime = time.time()\n",
    "lol=generate_captions(video_features, 20)\n",
    "endtime = time.time()\n",
    "for x in lol:\n",
    "  print(x)\n",
    "print(endtime-starttime)\n",
    "#print(\"!!\")\n",
    "#starttime = time.time()\n",
    "#lol=generate_captions_optimized(video_features, 20)\n",
    "#endtime = time.time()\n",
    "#for x in lol:\n",
    "#  print(x)\n",
    "#print(endtime-starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Meteor Score using nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9976851851851852\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  nltk.data.find('corpora\\wordnet')\n",
    "except LookupError:\n",
    "  nltk.download('wordnet')\n",
    "\n",
    "hypo = \"i am yogesh sharma from delhi\"\n",
    "ref = \"i am yogesh sharma from delhi\"\n",
    "score = nltk.translate.meteor_score.meteor_score([ref], hypo)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Bleu Score using nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 0.25 0.3333333333333333 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "hypo = \"i am yogesh sharma from delhi\".split()\n",
    "ref = \"i am yogesh sharma from delhi\".split()\n",
    "score = nltk.translate.bleu_score.sentence_bleu([ref], hypo)\n",
    "print(score)\n",
    "\n",
    "hypo = \"i am yogesh sharma\".split()\n",
    "ref = \"sharma yogesh am i\".split()\n",
    "bleu1 = nltk.translate.bleu_score.sentence_bleu([ref], hypo, weights=(1,0,0,0))\n",
    "bleu2 = nltk.translate.bleu_score.sentence_bleu([ref], hypo, (0,1,0,0), nltk.translate.bleu_score.SmoothingFunction().method2)\n",
    "bleu3 = nltk.translate.bleu_score.sentence_bleu([ref], hypo, (0,0,1,0), nltk.translate.bleu_score.SmoothingFunction().method2)\n",
    "bleu4 = nltk.translate.bleu_score.sentence_bleu([ref], hypo, (0,0,0,1), nltk.translate.bleu_score.SmoothingFunction().method2)\n",
    "print(bleu1, bleu2, bleu3, bleu4)\n",
    "# which smoothing function to use??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "class CNN():\n",
    "  def __init__(self):\n",
    "    model_directory = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    if not os.path.exists(model_directory):\n",
    "      vggmodel = VGG16(weights= 'imagenet')\n",
    "    else:\n",
    "      vggmodel = VGG16(weights= model_directory)\n",
    "    self.model = tf.keras.models.Model(vggmodel.input, vggmodel.layers[-2].output)\n",
    "\n",
    "  def __preprocess_frames(self, frames):\n",
    "    frames = np.array(list(map(lambda x: cv2.resize(x, (224,224)), frames)))\n",
    "    preprocessed_frames = preprocess_input(frames)\n",
    "    return preprocessed_frames\n",
    "  \n",
    "  def extract_features(self, frames):\n",
    "    preprocessed_frames = self.__preprocess_frames(frames)\n",
    "    features = self.model.predict(preprocessed_frames)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1010, 819, 3)\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002225E139AF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(86, 4096)\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "video_frames = get_random_video()\n",
    "print(video_frames.shape)\n",
    "features = cnn.extract_features(video_frames)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class Video_Caption_Generator():\n",
    "  def __init__(self):\n",
    "    \n",
    "    self.saved_model_directory = 'saved_models'\n",
    "    if not os.path.exists(os.path.join(self.saved_model_directory, 'encoder_model.h5')) or \\\n",
    "       not os.path.exists(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5')):\n",
    "      raise Exception(\"No trained models found. Check for correct model filenames if already trained, else train model first.\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(self.saved_model_directory, 'tokenizer.json')):\n",
    "      raise Exception(\"Tokenizer object not found.\")\n",
    "    \n",
    "    self.num_tokens_decoder = 5000\n",
    "    self.latent_dims = 1000\n",
    "    self.frames_limit = 80\n",
    "    \n",
    "    with open(os.path.join(self.saved_model_directory, 'tokenizer.json')) as fp:\n",
    "      tokenizer_json = json.load(fp)\n",
    "    \n",
    "    self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
    "    self.encoder_model, self.decoder_model = self.__get_inference_model()\n",
    "    self.cnn_model = CNN()\n",
    "    \n",
    "    self.caption_limit = 30\n",
    "    \n",
    "    self.testing_features_directory = 'dataset\\yt_allframes_vgg_fc7_test.txt'\n",
    "    self.testing_sents_directory = 'dataset\\sents_test_lc_nopunc.txt'\n",
    "    \n",
    "  def __get_inference_model(self):\n",
    "    e_model = tf.keras.models.load_model(os.path.join(self.saved_model_directory, 'encoder_model.h5'))\n",
    "\n",
    "    d_input = tf.keras.layers.Input(shape= (None, self.num_tokens_decoder))\n",
    "    d_input_h = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    d_input_c = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    d_input_state = [d_input_h, d_input_c]\n",
    "    d_lstm = tf.keras.layers.LSTM(self.latent_dims, return_state= True, return_sequences= True)\n",
    "    d_dense = tf.keras.layers.Dense(units= self.num_tokens_decoder, activation= 'softmax')\n",
    "    \n",
    "    d_output, d_state_h, d_state_c = d_lstm(d_input, initial_state= d_input_state)\n",
    "    d_output_state = [d_state_h, d_state_c]\n",
    "    d_output = d_dense(d_output)\n",
    "    d_model = tf.keras.models.Model(inputs= [d_input, d_input_state], outputs= [d_output, d_output_state])\n",
    "    d_model.load_weights(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5'))\n",
    "\n",
    "    return e_model, d_model\n",
    "  \n",
    "  def __convert_sequence_to_text(self, candidate_captions):\n",
    "    candidate_captions = list(map(lambda x: (x[0],x[1]), candidate_captions))\n",
    "    candidate_captions = sorted(candidate_captions, key= lambda x: x[0], reverse= True)\n",
    "    resulting_captions = []\n",
    "    resulting_probabilities = []\n",
    "    for candidate in candidate_captions:\n",
    "      sequence= candidate[1]\n",
    "      sequence= sequence[1:]\n",
    "      if self.tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "        sequence = sequence[:-1]\n",
    "      caption = self.tokenizer.sequences_to_texts([sequence])[0]\n",
    "      resulting_captions.append(caption)\n",
    "      resulting_probabilities.append(candidate[0])\n",
    "    \n",
    "    return resulting_captions, resulting_probabilities\n",
    "  \n",
    "  def __generate_captions(self, video_features, beam_width):\n",
    "    video_features = np.expand_dims(video_features, axis=0)\n",
    "    state_values = self.encoder_model.predict(video_features)\n",
    "    candidate_captions = [[0.0, [self.tokenizer.word_index['<bos>']], state_values]]\n",
    "    \n",
    "    for i in range(self.caption_limit):\n",
    "      new_candidates = []\n",
    "      all_done = 1\n",
    "      for candidate in candidate_captions:\n",
    "        prob = candidate[0]\n",
    "        sequence = candidate[1]\n",
    "        state = candidate[2]\n",
    "        if self.tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "          new_candidates.append([prob, sequence, state])\n",
    "          continue\n",
    "        all_done = 0\n",
    "        decoder_input_data = np.zeros(shape = (1, 1, self.num_tokens_decoder))\n",
    "        decoder_input_data[0, 0, sequence[-1]] = 1\n",
    "        probabilities, state_output = self.decoder_model.predict([decoder_input_data, state])\n",
    "        for i in range(1, self.num_tokens_decoder):\n",
    "          if probabilities[0, 0, i] > 0.0:\n",
    "            new_prob = prob + math.log(probabilities[0, 0, i])\n",
    "            new_sequence = sequence.copy()\n",
    "            new_sequence.append(i)\n",
    "            new_candidates.append([new_prob, new_sequence, state_output])\n",
    "      \n",
    "      if all_done == 1:\n",
    "        break\n",
    "      candidate_captions = sorted(new_candidates, key= lambda x: x[0], reverse= True)[:beam_width]\n",
    "        \n",
    "    return self.__convert_sequence_to_text(candidate_captions)\n",
    "  \n",
    "  def __generate_captions_optimized(self, video_features, beam_width):\n",
    "    video_features = np.expand_dims(video_features, axis=0)\n",
    "    state_values = self.encoder_model.predict(video_features)\n",
    "    candidate_captions = [[0.0, [self.tokenizer.word_index['<bos>']], state_values]]\n",
    "    \n",
    "    for i in range(self.caption_limit):\n",
    "      new_candidates = []\n",
    "      all_done = 1\n",
    "      for candidate in candidate_captions:\n",
    "        prob = candidate[0]\n",
    "        sequence = candidate[1]\n",
    "        state = candidate[2]\n",
    "        if self.tokenizer.index_word[sequence[-1]] == \"<eos>\":\n",
    "          if len(new_candidates) < beam_width:\n",
    "            heapq.heappush(new_candidates, (prob, sequence, state))\n",
    "          elif prob > new_candidates[0][0]:\n",
    "            heapq.heappushpop(new_candidates, (prob, sequence, state))\n",
    "          continue\n",
    "        all_done = 0\n",
    "        decoder_input_data = np.zeros(shape = (1, 1, self.num_tokens_decoder))\n",
    "        decoder_input_data[0, 0, sequence[-1]] = 1\n",
    "        probabilities, state_output = self.decoder_model.predict([decoder_input_data, state])\n",
    "        for j in range(1, self.num_tokens_decoder):\n",
    "          if probabilities[0, 0, j] > 0.0 and j != sequence[-1]:\n",
    "            new_prob = (prob*len(sequence) + math.log(probabilities[0, 0, j]))/(1+len(sequence))\n",
    "            new_sequence = sequence.copy()\n",
    "            new_sequence.append(j)\n",
    "            if len(new_candidates) < beam_width:\n",
    "              heapq.heappush(new_candidates, (new_prob, new_sequence, state_output))\n",
    "            elif new_prob > new_candidates[0][0]:\n",
    "              heapq.heappushpop(new_candidates, (new_prob, new_sequence, state_output))\n",
    "      \n",
    "      if all_done == 1:\n",
    "        break\n",
    "      candidate_captions = new_candidates.copy()\n",
    "    \n",
    "    return self.__convert_sequence_to_text(candidate_captions)\n",
    "  \n",
    "  def __fine_tune_captions(self, captions):\n",
    "    result = []\n",
    "    for caption in captions:\n",
    "      cap_arr = caption.split()\n",
    "      new_cap_arr = []\n",
    "      for word in cap_arr:\n",
    "        if len(new_cap_arr) == 0:\n",
    "          new_cap_arr.append(word)\n",
    "        elif word != new_cap_arr[-1]:\n",
    "          new_cap_arr.append(word)\n",
    "      new_caption = ' '.join(new_cap_arr)\n",
    "      result.append(new_caption)\n",
    "    return result\n",
    "  \n",
    "  def get_video_captions(self, video_directory, beam_width=5):\n",
    "    \n",
    "    if not os.path.exists(video_directory):\n",
    "      raise Exception(\"Invalid Video directory\")\n",
    "    \n",
    "    print(\"Reading Video Frames...\")\n",
    "    cap = cv2.VideoCapture(video_directory)\n",
    "    \n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    if frames.shape[0] > self.frames_limit:\n",
    "      frames_idx = np.linspace(0, frames.shape[0]-1, self.frames_limit).astype('int')\n",
    "      frames = frames[frames_idx]\n",
    "    \n",
    "    print(\"Extracting Features...\")\n",
    "    features = self.cnn_model.extract_features(frames)\n",
    "    features = np.pad(features, ((0,self.frames_limit - features.shape[0]), (0,0)))\n",
    "    \n",
    "    print(\"Generating Captions...\")\n",
    "    #captions, probabilities = self.__generate_captions(features, beam_width)\n",
    "    captions, probabilities = self.__generate_captions_optimized(features, beam_width)\n",
    "\n",
    "    #captions = self.__fine_tune_captions(captions)\n",
    "    \n",
    "    return captions\n",
    "  \n",
    "  def get_bleu_score(self, references, hypothesis):\n",
    "    references = list(map(lambda x: x.split(), references))\n",
    "    hypothesis = list(map(lambda x: x.split(), hypothesis))\n",
    "    bleu_scores = []\n",
    "    for hypo in hypothesis:\n",
    "      score = nltk.translate.bleu_score.sentence_bleu(references, hypo)\n",
    "      bleu_scores.append(score)\n",
    "    return max(bleu_scores)\n",
    "  \n",
    "  def get_meteor_score(self, references, hypothesis):\n",
    "    try:\n",
    "      nltk.data.find('corpora\\wordnet')\n",
    "    except LookupError:\n",
    "      nltk.download('wordnet')\n",
    "      \n",
    "    meteor_scores = []\n",
    "    for hypo in hypothesis:\n",
    "      score = nltk.translate.meteor_score.meteor_score(references, hypo)\n",
    "      meteor_scores.append(score)\n",
    "    return max(meteor_scores)\n",
    "  \n",
    "  def __getFeatures(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split()\n",
    "    features = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split(',')\n",
    "      id = row[0].split('_')[0]\n",
    "      if id not in features:\n",
    "        features[id]=[]\n",
    "      features[id].append(np.asarray(row[1:], dtype=np.float))\n",
    "\n",
    "    for x in features:\n",
    "      features[x] = np.array(features[x])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "  def __getSents(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split('\\n')\n",
    "    sents = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split('\\t')\n",
    "      if len(row)<2:\n",
    "        continue\n",
    "      id = row[0]\n",
    "      if id not in sents:\n",
    "        sents[id] = []\n",
    "      sents[id].append(row[1])\n",
    "\n",
    "    return sents\n",
    "  \n",
    "  def test(self):\n",
    "    testing_features = self.__getFeatures(self.testing_features_directory)\n",
    "    testing_sents = self.__getSents(self.testing_sents_directory)\n",
    "    for key in testing_features:\n",
    "      vid_features = testing_features[key]\n",
    "      if vid_features.shape[0] < self.frames_limit:\n",
    "        vid_features = np.pad(vid_features, ((0, self.frames_limit-vid_features.shape[0]), (0,0)))\n",
    "      else:\n",
    "        frames_idx = np.linspace(0, vid_features.shape[0]-1, self.frames_limit).astype('int')\n",
    "        vid_features = vid_features[frames_idx]\n",
    "      testing_features[key] = vid_features\n",
    "    \n",
    "    bleu = []\n",
    "    meteor = []\n",
    "    for beam_width in range(1,21):\n",
    "      print(\"!!\",beam_width)\n",
    "      bleu_temp = []\n",
    "      meteor_temp = []\n",
    "      for key, features in testing_features.items():\n",
    "        captions = self.__generate_captions_optimized(features, beam_width)[0]\n",
    "        bleu_score = self.get_bleu_score(testing_sents[key], captions)\n",
    "        meteor_score = self.get_meteor_score(testing_sents[key], captions)\n",
    "        bleu_temp.append(bleu_score)\n",
    "        meteor_temp.append(meteor_score)\n",
    "      bleu.append(np.average(bleu_temp))\n",
    "      meteor.append(np.average(meteor_temp))\n",
    "    return bleu, meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "vcg = Video_Caption_Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.999898229187869\n"
     ]
    }
   ],
   "source": [
    "s1=[\"i am yogesh sharma from new delhi india and i do not know what i will do\", \"i am sharma yogesh from nsit\"]\n",
    "h1=[\"i am yogesh sharma from new delhi india and i do not know what i will do\", \"i am yogesh sharma and something is strange\"]\n",
    "\n",
    "print(vcg.get_bleu_score(s1, h1))\n",
    "print(vcg.get_meteor_score(s1,h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Video Frames...\n",
      "Extracting Features...\n",
      "Generating Captions...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the two thief', 'a man is playing', 'a man is playing a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppp = r'C:\\Users\\Yogesh\\Desktop\\Projects\\Video Captioning\\dataset\\demo\\12501.gif'\n",
    "vcg.get_video_captions(ppp, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Yogesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! 2\n",
      "!! 3\n",
      "!! 4\n",
      "!! 5\n",
      "!! 6\n",
      "!! 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6c30a2166681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeteor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvcg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-29c5487f2a04>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0mmeteor_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtesting_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__generate_captions_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mbleu_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bleu_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_sents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mmeteor_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_meteor_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_sents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-29c5487f2a04>\u001b[0m in \u001b[0;36m__generate_captions_optimized\u001b[1;34m(self, video_features, beam_width)\u001b[0m\n\u001b[0;32m    119\u001b[0m           \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mall_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mdecoder_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_tokens_decoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bleu, meteor = vcg.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Video Frames...\n",
      "Extracting Features...\n",
      "Generating Captions...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a girl is riding a on a in a in a in a in a',\n",
       " 'a girl is riding a in a in a in a in a in a',\n",
       " 'a girl is riding a the a in a in a in a in a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppp = r'C:\\Users\\Yogesh\\Desktop\\Projects\\Video Captioning\\dataset\\demo\\2.gif'\n",
    "vcg.get_video_captions(ppp, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, batch_size, features, video_ids, cap_sequences, vocab_size, max_length):\n",
    "    self.batch_size = batch_size\n",
    "    self.features = features\n",
    "    self.video_ids = video_ids\n",
    "    self.cap_sequences = cap_sequences\n",
    "    self.vocab_size = vocab_size\n",
    "    self.max_len = max_length\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indices = np.arange(len(self.cap_sequences))\n",
    "    np.random.shuffle(self.indices)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.indices)//self.batch_size\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    idx_range = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
    "    return self.__data_generation(idx_range)\n",
    "  \n",
    "  def __data_generation(self, list_index):\n",
    "    X1=[]\n",
    "    X2=[]\n",
    "    Y=[]\n",
    "    for i in list_index:\n",
    "      id=self.video_ids[i]\n",
    "      X1.append(self.features[id])\n",
    "      X2.append(tf.keras.utils.to_categorical(self.cap_sequences[i][:-1],num_classes= self.vocab_size))\n",
    "      Y.append(tf.keras.utils.to_categorical(self.cap_sequences[i][1:],num_classes= self.vocab_size))\n",
    "    X1=np.array(X1)\n",
    "    X2=tf.keras.preprocessing.sequence.pad_sequences(X2, maxlen= self.max_len, padding='post')\n",
    "    Y=tf.keras.preprocessing.sequence.pad_sequences(Y, maxlen= self.max_len, padding='post')\n",
    "    return [X1,X2], Y\n",
    "\n",
    "\n",
    "\n",
    "class Video_Captioning_Model():\n",
    "  def __init__(self, ):\n",
    "    \n",
    "    validation_features_directory = 'dataset\\yt_allframes_vgg_fc7_val.txt'\n",
    "    validation_sents_directory = 'dataset\\sents_val_lc_nopunc.txt'\n",
    "    \n",
    "    print(\"Loading Dataset...\")\n",
    "    raw_validation_features = self.__getFeatures(validation_features_directory)\n",
    "    raw_validation_sents = self.__getSents(validation_sents_directory)\n",
    "    \n",
    "    self.frames_limit = 80\n",
    "    self.vocab_size = 1500\n",
    "    \n",
    "    self.saved_model_directory = 'saved_models'\n",
    "    \n",
    "    self.validation_features, self.validation_ids, self.validation_captions = self.preprocess_data(raw_validation_features, \n",
    "                                                                                                   raw_validation_sents)\n",
    "    \n",
    "    self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words= self.vocab_size,\n",
    "                                                          filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                          oov_token= \"<oov>\")\n",
    "    self.tokenizer.fit_on_texts(self.validation_captions)\n",
    "    self.validation_seq = self.tokenizer.texts_to_sequences(self.validation_captions)\n",
    "    self.max_length = max([len(x) for x in self.validation_seq])\n",
    "    \n",
    "    tokenizer_json = self.tokenizer.to_json()\n",
    "    with open(os.path.join(self.saved_model_directory, 'tokenizer.json'), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "    self.num_timesteps_encoder = 80\n",
    "    self.num_tokens_encoder = 4096\n",
    "    self.latent_dims = 512\n",
    "    self.num_timesteps_decoder = self.max_length - 1\n",
    "    self.num_tokens_decoder = self.vocab_size\n",
    "    \n",
    "    return\n",
    "  \n",
    "  def train(self):\n",
    "    self.model = self.build_model()\n",
    "    self.model.compile(\n",
    "      optimizer= 'adam',\n",
    "      loss= 'categorical_crossentropy',\n",
    "      metrics= ['accuracy']\n",
    "    )\n",
    "    \n",
    "    training_generator = DataGenerator(\n",
    "      batch_size= 32,\n",
    "      features= self.validation_features,\n",
    "      video_ids= self.validation_ids,\n",
    "      cap_sequences= self.validation_seq,\n",
    "      max_length= self.max_length-1,\n",
    "      vocab_size= self.vocab_size\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    try:\n",
    "      self.history = self.model.fit(\n",
    "        training_generator,\n",
    "        epochs= 1\n",
    "      )\n",
    "    except KeyboardInterrupt:\n",
    "      print(\"Keyboard interrupt!!\")\n",
    "    \n",
    "    print(\"Saving Models...\")\n",
    "    self.encoder_model = tf.keras.models.Model(self.encoder_input, self.encoder_state)\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape= (self.latent_dims,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_output_lstm, state_h, state_c = self.decoder_lstm(self.decoder_input, initial_state= decoder_state_inputs)\n",
    "    decoder_state_outputs = [state_h, state_c]\n",
    "    decoder_output_dense = self.decoder_dense(decoder_output_lstm)\n",
    "    self.decoder_model = tf.keras.models.Model(inputs=[self.decoder_input, decoder_state_inputs], outputs=[decoder_output_dense,decoder_state_outputs])\n",
    "\n",
    "    self.encoder_model.save(os.path.join(self.saved_model_directory, 'encoder_model.h5'))\n",
    "    self.decoder_model.save_weights(os.path.join(self.saved_model_directory, 'decoder_model_weights.h5'))\n",
    "    np.save(os.path.join(self.saved_model_directory, 'history.npy'), self.history.history)\n",
    "  \n",
    "  def build_model(self):\n",
    "    print(\"Building Model...\")\n",
    "    \n",
    "    self.encoder_input = tf.keras.layers.Input(shape = (self.num_timesteps_encoder, self.num_tokens_encoder), name= 'encoder_inputs')\n",
    "    self.encoder_lstm = tf.keras.layers.LSTM(units= self.latent_dims, return_state= True, return_sequences= True, name= 'encoder_lstm')\n",
    "    _, state_h, state_c = self.encoder_lstm(self.encoder_input)\n",
    "    self.encoder_state = [state_h, state_c]\n",
    "    \n",
    "    self.decoder_input = tf.keras.layers.Input(shape = (self.num_timesteps_decoder, self.num_tokens_decoder), name= 'decoder_inputs')\n",
    "    self.decoder_lstm = tf.keras.layers.LSTM(units= self.latent_dims, return_state= True, return_sequences= True, name= 'decoder_lstm')\n",
    "    self.decoder_output, _, _ = self.decoder_lstm(self.decoder_input, initial_state= self.encoder_state)\n",
    "    self.decoder_dense = tf.keras.layers.Dense(units= self.vocab_size, activation= 'softmax', name= 'decoder_dense')\n",
    "    self.decoder_output = self.decoder_dense(self.decoder_output)\n",
    "    \n",
    "    model = tf.keras.models.Model([self.encoder_input, self.decoder_input], self.decoder_output)\n",
    "    return model\n",
    "  \n",
    "  def preprocess_data(self, features, sents):\n",
    "    result_features = {}\n",
    "    result_ids = []\n",
    "    result_captions = []\n",
    "    \n",
    "    for x,y in features.items():\n",
    "      video_features = None\n",
    "      if y.shape[0] > self.frames_limit:\n",
    "        idx = np.linspace(0, y.shape[0]-1, self.frames_limit).astype('int')\n",
    "        video_features = y[idx]\n",
    "      else:\n",
    "        video_features = np.pad(y, ((0, self.frames_limit - y.shape[0]), (0, 0)))\n",
    "      result_features[x]=video_features\n",
    "    \n",
    "    for id, captions in sents.items():\n",
    "      for caption in captions:\n",
    "        result_ids.append(id)\n",
    "        cap = \"<bos> \" + caption + \" <eos>\"\n",
    "        result_captions.append(cap)\n",
    "    \n",
    "    return result_features, result_ids, result_captions\n",
    "  \n",
    "  def __getFeatures(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split()\n",
    "    features = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split(',')\n",
    "      id = row[0].split('_')[0]\n",
    "      if id not in features:\n",
    "        features[id]=[]\n",
    "      features[id].append(np.asarray(row[1:], dtype=np.float))\n",
    "\n",
    "    for x in features:\n",
    "      features[x] = np.array(features[x])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "  def __getSents(self, directory):\n",
    "    with open(directory, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "    datalist = data.split('\\n')\n",
    "    sents = {}\n",
    "\n",
    "    for x in datalist:\n",
    "      row = x.split('\\t')\n",
    "      if len(row)<2:\n",
    "        continue\n",
    "      id = row[0]\n",
    "      if id not in sents:\n",
    "        sents[id] = []\n",
    "      sents[id].append(row[1])\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcm = Video_Captioning_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 560s 4s/step - loss: 1.2593 - accuracy: 0.0871\n"
     ]
    }
   ],
   "source": [
    "vcm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = r'C:\\Users\\Yogesh\\Desktop\\Projects\\Video Captioning\\dataset\\demo\\inputVideo1cut.mp4'\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "while cap.isOpened():\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    break\n",
    "  cv2.imshow('video',frame)\n",
    "  if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bitcfd7c34c2f954e0c9a347e48585b174c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
